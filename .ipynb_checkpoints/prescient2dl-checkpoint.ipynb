{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "def kill_processes_by_name(process_name):\n",
    "    for proc in psutil.process_iter(['pid', 'name']):\n",
    "        if proc.info['name'] == process_name:\n",
    "            pid = proc.info['pid']\n",
    "            try:\n",
    "                process = psutil.Process(pid)\n",
    "                process.terminate()\n",
    "                print(f\"Process with PID {pid} terminated.\")\n",
    "            except psutil.NoSuchProcess:\n",
    "                print(f\"Process with PID {pid} no longer exists.\")\n",
    "\n",
    "# Example usage\n",
    "pid_name = \"tensorboard.exe\"\n",
    "kill_processes_by_name(pid_name)\n",
    "\n",
    "# Get directory name\n",
    "mydir = \"C:\\\\Users\\\\antho\\\\AppData\\\\Local\\\\Temp\\\\.tensorboard-info\\\\\"\n",
    "\n",
    "# Try to remove the tree; if it fails, throw an error using try...except.\n",
    "try:\n",
    "    shutil.rmtree(mydir)\n",
    "except OSError as e:\n",
    "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ao7fJW1Pyiza"
   },
   "outputs": [],
   "source": [
    "# # Clear any logs from previous runs\n",
    "# !rmdir /S /Q \"./doc/_tensorboard_logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6B95Hb6YVgPZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6B95Hb6YVgPZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['tensorboard', '--logdir', 'logs/fit']>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "logdir = 'logs/fit'\n",
    "command = f\"tensorboard --logdir {logdir}\"\n",
    "subprocess.Popen(command.split(), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "# %tensorboard --logdir logs/fit;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "\n",
    "# Define the URL\n",
    "url = \"http://localhost:6006/?darkMode=true\"\n",
    "\n",
    "# Open the URL in the default browser\n",
    "webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START OF PRESCIENT2DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import keras\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from lib import custom_architectures\n",
    "import visualkeras\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.metrics import MeanAbsolutePercentageError, MeanAbsoluteError, MeanSquaredError\n",
    "from keras.models import load_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS\n",
    "directory = \"F:\\\\\"\n",
    "num_epochs = 100\n",
    "max_batch_size = 128*32\n",
    "sample = np.load('F:\\\\instances_output_0000000000-0000004999\\\\instance_0000000000_o.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6B95Hb6YVgPZ"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n",
      "channels_last\n"
     ]
    }
   ],
   "source": [
    "# print current format\n",
    "print(K.image_data_format())\n",
    "# set format\n",
    "K.set_image_data_format('channels_last')\n",
    "print(K.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(max_batch_size/32)\n",
    "\n",
    "folders = [f for f in os.listdir(directory) if os.path.isdir(os.path.join(directory, f)) and \"instances_output_0\" in f]\n",
    "selected_folders = [\"instances_output_0000000000-0000000999\"]\n",
    "selected_folders = [folders[0]]\n",
    "# selected_folders.remove('instances_output_0000030000-0000034999')\n",
    "# selected_folders.remove('instances_output_0000025000-0000029999')\n",
    "# selected_folders.remove('instances_output_0000020000-0000024999')\n",
    "# selected_folders.remove('instances_output_0000015000-0000019999')\n",
    "# selected_folders.remove('instances_output_0000010000-0000014999')\n",
    "# selected_folders.remove('instances_output_0000005000-0000009999')\n",
    "# selected_folders.remove('instances_output_0000000000-0000004999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_folders:  ['instances_output_0000000000-0000004999']\n"
     ]
    }
   ],
   "source": [
    "print(\"selected_folders: \", selected_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "N1 = sample.shape[1]\n",
    "N2 = sample.shape[2]\n",
    "# input_shape = (2, N1, N2)\n",
    "input_shape = (N1, N2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory:  F:\\\n"
     ]
    }
   ],
   "source": [
    "print(\"directory: \", directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "class PlotTrainingHistory(Callback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.history = {'loss': [], 'val_loss': []}\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        if os.path.exists('training_history.pkl'):\n",
    "            with open('training_history.pkl', 'rb') as file:\n",
    "                self.history = pickle.load(file)\n",
    "                self.losses = history['loss']\n",
    "                self.val_losses = history['val_loss']\n",
    "        else:\n",
    "            self.losses = []\n",
    "            self.val_losses = []\n",
    "            self.history = {'loss': [], 'val_loss': []}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.history['loss'].append(logs.get('loss'))\n",
    "        self.history['val_loss'].append(logs.get('val_loss'))\n",
    "        self.save_model_and_history()\n",
    "        # self.plot()\n",
    "\n",
    "    def plot(self):\n",
    "        plt.figure()\n",
    "        plt.plot(self.losses, label='train_loss')\n",
    "        plt.plot(self.val_losses, label='val_loss')\n",
    "        plt.title('Training History')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def save_model_and_history(self):\n",
    "        self.model.save('model_checkpoint.h5')\n",
    "        with open('training_history.pkl', 'wb') as file:\n",
    "            pickle.dump(self.history, file)\n",
    "            \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_folder:  F:\\instances_output_0000000000-0000004999\n",
      "Loading Data Splits\n",
      "sets loaded: train\n",
      "sets loaded: validation\n",
      "sets loaded: test\n",
      "samples training:  3200\n",
      "samples testing:  800\n",
      "samples validation:  1000\n",
      "steps_per_epoch: 25\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 128, 128, 2  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 64, 64, 128)  1152        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 64, 64, 128)  512        ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 64, 64, 128)  0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " separable_conv2d (SeparableCon  (None, 64, 64, 8)   2184        ['activation[0][0]']             \n",
      " v2D)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 64, 64, 8)   32          ['separable_conv2d[0][0]']       \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 64, 64, 8)    0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_1 (SeparableC  (None, 64, 64, 8)   144         ['activation_1[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 64, 64, 8)   32          ['separable_conv2d_1[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 32, 32, 8)    0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 8)    1032        ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 8)    0           ['max_pooling2d[0][0]',          \n",
      "                                                                  'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 8)    0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " separable_conv2d_2 (SeparableC  (None, 32, 32, 16)  216         ['activation_2[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 16)  64          ['separable_conv2d_2[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_3 (SeparableC  (None, 32, 32, 16)  416         ['activation_3[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 16)  64          ['separable_conv2d_3[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 16)  0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 16, 16, 16)   144         ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 16, 16, 16)   0           ['max_pooling2d_1[0][0]',        \n",
      "                                                                  'conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 16, 16, 16)   0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " separable_conv2d_4 (SeparableC  (None, 16, 16, 32)  688         ['activation_4[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 16, 16, 32)  128         ['separable_conv2d_4[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 16, 16, 32)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_5 (SeparableC  (None, 16, 16, 32)  1344        ['activation_5[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 16, 16, 32)  128         ['separable_conv2d_5[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 32)    0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 8, 8, 32)     544         ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 8, 8, 32)     0           ['max_pooling2d_2[0][0]',        \n",
      "                                                                  'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 8, 8, 32)     0           ['add_2[0][0]']                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " separable_conv2d_6 (SeparableC  (None, 8, 8, 64)    2400        ['activation_6[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 8, 8, 64)    256         ['separable_conv2d_6[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 8, 8, 64)     0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_7 (SeparableC  (None, 8, 8, 64)    4736        ['activation_7[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 8, 8, 64)    256         ['separable_conv2d_7[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 4, 4, 64)    0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 4, 4, 64)     2112        ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4, 4, 64)     0           ['max_pooling2d_3[0][0]',        \n",
      "                                                                  'conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 4, 4, 64)     0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " separable_conv2d_8 (SeparableC  (None, 4, 4, 128)   8896        ['activation_8[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 4, 4, 128)   512         ['separable_conv2d_8[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 4, 4, 128)    0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_9 (SeparableC  (None, 4, 4, 128)   17664       ['activation_9[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 4, 4, 128)   512         ['separable_conv2d_9[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 2, 2, 128)   0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 2, 2, 128)    8320        ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 2, 2, 128)    0           ['max_pooling2d_4[0][0]',        \n",
      "                                                                  'conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 2, 2, 128)    0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 2, 2, 128)   147584      ['activation_10[0][0]']          \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 2, 2, 128)   512         ['conv2d_transpose[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 2, 2, 128)    0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 2, 2, 128)   147584      ['activation_11[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 2, 2, 128)   512         ['conv2d_transpose_1[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 4, 4, 128)   0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 4, 4, 128)    0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 4, 4, 128)    16512       ['up_sampling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4, 4, 128)    0           ['up_sampling2d[0][0]',          \n",
      "                                                                  'conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 4, 4, 128)    0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 4, 4, 64)    73792       ['activation_12[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 4, 4, 64)    256         ['conv2d_transpose_2[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 4, 4, 64)     0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 4, 4, 64)    36928       ['activation_13[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_14 (BatchN  (None, 4, 4, 64)    256         ['conv2d_transpose_3[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " up_sampling2d_3 (UpSampling2D)  (None, 8, 8, 128)   0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSampling2D)  (None, 8, 8, 64)    0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 8, 8, 64)     8256        ['up_sampling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 8, 8, 64)     0           ['up_sampling2d_2[0][0]',        \n",
      "                                                                  'conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 8, 8, 64)     0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_4 (Conv2DTran  (None, 8, 8, 32)    18464       ['activation_14[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 8, 8, 32)    128         ['conv2d_transpose_4[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 8, 8, 32)     0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_5 (Conv2DTran  (None, 8, 8, 32)    9248        ['activation_15[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 8, 8, 32)    128         ['conv2d_transpose_5[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " up_sampling2d_5 (UpSampling2D)  (None, 16, 16, 64)  0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " up_sampling2d_4 (UpSampling2D)  (None, 16, 16, 32)  0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 16, 16, 32)   2080        ['up_sampling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 16, 16, 32)   0           ['up_sampling2d_4[0][0]',        \n",
      "                                                                  'conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 16, 16, 32)   0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_6 (Conv2DTran  (None, 16, 16, 16)  4624        ['activation_16[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 16, 16, 16)  64          ['conv2d_transpose_6[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 16, 16, 16)   0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_7 (Conv2DTran  (None, 16, 16, 16)  2320        ['activation_17[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 16, 16, 16)  64          ['conv2d_transpose_7[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " up_sampling2d_7 (UpSampling2D)  (None, 32, 32, 32)  0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " up_sampling2d_6 (UpSampling2D)  (None, 32, 32, 16)  0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 32, 32, 16)   528         ['up_sampling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 32, 32, 16)   0           ['up_sampling2d_6[0][0]',        \n",
      "                                                                  'conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 32, 32, 16)   0           ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_8 (Conv2DTran  (None, 32, 32, 8)   1160        ['activation_18[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 32, 32, 8)   32          ['conv2d_transpose_8[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 32, 32, 8)    0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_9 (Conv2DTran  (None, 32, 32, 8)   584         ['activation_19[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 32, 32, 8)   32          ['conv2d_transpose_9[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " up_sampling2d_9 (UpSampling2D)  (None, 64, 64, 16)  0           ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " up_sampling2d_8 (UpSampling2D)  (None, 64, 64, 8)   0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 64, 64, 8)    136         ['up_sampling2d_9[0][0]']        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 64, 64, 8)    0           ['up_sampling2d_8[0][0]',        \n",
      "                                                                  'conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 64, 64, 8)    0           ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_10 (Conv2DTra  (None, 64, 64, 4)   292         ['activation_20[0][0]']          \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 64, 64, 4)   16          ['conv2d_transpose_10[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 64, 64, 4)    0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_11 (Conv2DTra  (None, 64, 64, 4)   148         ['activation_21[0][0]']          \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 64, 64, 4)   16          ['conv2d_transpose_11[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " up_sampling2d_11 (UpSampling2D  (None, 128, 128, 8)  0          ['add_9[0][0]']                  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " up_sampling2d_10 (UpSampling2D  (None, 128, 128, 4)  0          ['batch_normalization_22[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 128, 128, 4)  36          ['up_sampling2d_11[0][0]']       \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 128, 128, 4)  0           ['up_sampling2d_10[0][0]',       \n",
      "                                                                  'conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 128, 128, 2)  74          ['add_10[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 526,854\n",
      "Trainable params: 524,598\n",
      "Non-trainable params: 2,256\n",
      "__________________________________________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "File exists!\n",
      "File exists!\n",
      "Epoch 1/100\n",
      " 6/25 [======>.......................] - ETA: 2s - loss: 0.0690 - mean_squared_error: 0.0690WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0235s vs `on_train_batch_end` time: 0.0876s). Check your callbacks.\n",
      "25/25 [==============================] - 11s 219ms/step - loss: 0.0463 - mean_squared_error: 0.0463 - val_loss: 0.0366 - val_mean_squared_error: 0.0366\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 4s 176ms/step - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0270 - val_mean_squared_error: 0.0270\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 4s 176ms/step - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0258 - val_mean_squared_error: 0.0258\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 4s 180ms/step - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0251 - val_mean_squared_error: 0.0251\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 4s 183ms/step - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0248 - val_mean_squared_error: 0.0248\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 4s 171ms/step - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0250 - val_mean_squared_error: 0.0250\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 4s 181ms/step - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0261 - val_mean_squared_error: 0.0261\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 4s 182ms/step - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0258 - val_mean_squared_error: 0.0258\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0268 - val_mean_squared_error: 0.0268\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 4s 173ms/step - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0277 - val_mean_squared_error: 0.0277\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 4s 175ms/step - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0275 - val_mean_squared_error: 0.0275\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 4s 178ms/step - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0254 - val_mean_squared_error: 0.0254\n",
      "Epoch 13/100\n",
      "25/25 [==============================] - 4s 176ms/step - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0266 - val_mean_squared_error: 0.0266\n",
      "Epoch 14/100\n",
      "25/25 [==============================] - 4s 177ms/step - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
      "Epoch 15/100\n",
      "25/25 [==============================] - 5s 192ms/step - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0265 - val_mean_squared_error: 0.0265\n",
      "Epoch 16/100\n",
      "25/25 [==============================] - 5s 190ms/step - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0283 - val_mean_squared_error: 0.0283\n",
      "Epoch 17/100\n",
      "25/25 [==============================] - 5s 199ms/step - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0268 - val_mean_squared_error: 0.0268\n",
      "Epoch 18/100\n",
      "25/25 [==============================] - 5s 218ms/step - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0265 - val_mean_squared_error: 0.0265\n",
      "Epoch 19/100\n",
      "25/25 [==============================] - 5s 214ms/step - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0263 - val_mean_squared_error: 0.0263\n",
      "Epoch 20/100\n",
      "25/25 [==============================] - 5s 221ms/step - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0265 - val_mean_squared_error: 0.0265\n",
      "Epoch 21/100\n",
      "25/25 [==============================] - 5s 211ms/step - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0255 - val_mean_squared_error: 0.0255\n",
      "Epoch 22/100\n",
      "25/25 [==============================] - 6s 225ms/step - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0256 - val_mean_squared_error: 0.0256\n",
      "Epoch 23/100\n",
      "25/25 [==============================] - 6s 229ms/step - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0250 - val_mean_squared_error: 0.0250\n",
      "Epoch 24/100\n",
      "25/25 [==============================] - 5s 221ms/step - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0251 - val_mean_squared_error: 0.0251\n",
      "Epoch 25/100\n",
      "25/25 [==============================] - 6s 243ms/step - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0253 - val_mean_squared_error: 0.0253\n",
      "Epoch 26/100\n",
      "25/25 [==============================] - 6s 235ms/step - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0254 - val_mean_squared_error: 0.0254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "25/25 [==============================] - 6s 237ms/step - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0265 - val_mean_squared_error: 0.0265\n",
      "Epoch 28/100\n",
      "25/25 [==============================] - 6s 234ms/step - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
      "Epoch 29/100\n",
      "25/25 [==============================] - 6s 234ms/step - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0253 - val_mean_squared_error: 0.0253\n",
      "Epoch 30/100\n",
      "25/25 [==============================] - 6s 235ms/step - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0257 - val_mean_squared_error: 0.0257\n",
      "Epoch 31/100\n",
      "25/25 [==============================] - 6s 232ms/step - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0255 - val_mean_squared_error: 0.0255\n",
      "Epoch 32/100\n",
      "25/25 [==============================] - 6s 227ms/step - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
      "Epoch 33/100\n",
      "25/25 [==============================] - 6s 227ms/step - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0280 - val_mean_squared_error: 0.0280\n",
      "Epoch 34/100\n",
      "25/25 [==============================] - 6s 230ms/step - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0283 - val_mean_squared_error: 0.0283\n",
      "Epoch 35/100\n",
      "25/25 [==============================] - 6s 232ms/step - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0273 - val_mean_squared_error: 0.0273\n",
      "Epoch 36/100\n",
      "25/25 [==============================] - 6s 229ms/step - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0265 - val_mean_squared_error: 0.0265\n",
      "Epoch 37/100\n",
      "25/25 [==============================] - 6s 226ms/step - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
      "Epoch 38/100\n",
      "25/25 [==============================] - 6s 226ms/step - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0258 - val_mean_squared_error: 0.0258\n",
      "Epoch 39/100\n",
      "25/25 [==============================] - 6s 227ms/step - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0253 - val_mean_squared_error: 0.0253\n",
      "Epoch 40/100\n",
      "25/25 [==============================] - 6s 233ms/step - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0256 - val_mean_squared_error: 0.0256\n",
      "Epoch 41/100\n",
      "25/25 [==============================] - 6s 234ms/step - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0258 - val_mean_squared_error: 0.0258\n",
      "Epoch 42/100\n",
      "25/25 [==============================] - 6s 240ms/step - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0254 - val_mean_squared_error: 0.0254\n",
      "Epoch 43/100\n",
      "25/25 [==============================] - 6s 235ms/step - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
      "Epoch 44/100\n",
      "25/25 [==============================] - 6s 233ms/step - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0263 - val_mean_squared_error: 0.0263\n",
      "Epoch 45/100\n",
      "25/25 [==============================] - 6s 237ms/step - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0258 - val_mean_squared_error: 0.0258\n",
      "Epoch 46/100\n",
      "25/25 [==============================] - 6s 233ms/step - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0272 - val_mean_squared_error: 0.0272\n",
      "Epoch 47/100\n",
      "25/25 [==============================] - 6s 240ms/step - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0261 - val_mean_squared_error: 0.0261\n",
      "Epoch 48/100\n",
      "25/25 [==============================] - 7s 281ms/step - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0273 - val_mean_squared_error: 0.0273\n",
      "Epoch 49/100\n",
      "25/25 [==============================] - 6s 251ms/step - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0257 - val_mean_squared_error: 0.0257\n",
      "Epoch 50/100\n",
      "25/25 [==============================] - 6s 235ms/step - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0253 - val_mean_squared_error: 0.0253\n",
      "Epoch 51/100\n",
      "25/25 [==============================] - 6s 250ms/step - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0254 - val_mean_squared_error: 0.0254\n",
      "Epoch 52/100\n",
      "25/25 [==============================] - 6s 242ms/step - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0249 - val_mean_squared_error: 0.0249\n",
      "Epoch 53/100\n",
      "25/25 [==============================] - 6s 260ms/step - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0253 - val_mean_squared_error: 0.0253\n",
      "Epoch 54/100\n",
      "25/25 [==============================] - 6s 254ms/step - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0264 - val_mean_squared_error: 0.0264\n",
      "Epoch 55/100\n",
      "25/25 [==============================] - 7s 267ms/step - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0260 - val_mean_squared_error: 0.0260\n",
      "Epoch 56/100\n",
      "25/25 [==============================] - 7s 292ms/step - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0276 - val_mean_squared_error: 0.0276\n",
      "Epoch 57/100\n",
      "25/25 [==============================] - 9s 348ms/step - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0264 - val_mean_squared_error: 0.0264\n",
      "Epoch 58/100\n",
      "25/25 [==============================] - 7s 275ms/step - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0271 - val_mean_squared_error: 0.0271\n",
      "Epoch 59/100\n",
      "25/25 [==============================] - 7s 299ms/step - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0261 - val_mean_squared_error: 0.0261\n",
      "Epoch 60/100\n",
      "25/25 [==============================] - 7s 265ms/step - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
      "Epoch 61/100\n",
      "25/25 [==============================] - 7s 299ms/step - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0253 - val_mean_squared_error: 0.0253\n",
      "Epoch 62/100\n",
      "25/25 [==============================] - 9s 361ms/step - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0274 - val_mean_squared_error: 0.0274\n",
      "Epoch 63/100\n",
      "25/25 [==============================] - 7s 267ms/step - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0256 - val_mean_squared_error: 0.0256\n",
      "Epoch 64/100\n",
      "25/25 [==============================] - 9s 365ms/step - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0264 - val_mean_squared_error: 0.0264\n",
      "Epoch 65/100\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0256 - val_mean_squared_error: 0.0256\n",
      "Epoch 66/100\n",
      "25/25 [==============================] - 6s 241ms/step - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0267 - val_mean_squared_error: 0.0267\n",
      "Epoch 67/100\n",
      "25/25 [==============================] - 10s 392ms/step - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0253 - val_mean_squared_error: 0.0253\n",
      "Epoch 68/100\n",
      "25/25 [==============================] - 7s 303ms/step - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0258 - val_mean_squared_error: 0.0258\n",
      "Epoch 69/100\n",
      "25/25 [==============================] - 7s 271ms/step - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0274 - val_mean_squared_error: 0.0274\n",
      "Epoch 70/100\n",
      "25/25 [==============================] - 7s 282ms/step - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
      "Epoch 71/100\n",
      "25/25 [==============================] - 10s 428ms/step - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0262 - val_mean_squared_error: 0.0262\n",
      "Epoch 72/100\n",
      "25/25 [==============================] - 7s 267ms/step - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
      "Epoch 73/100\n",
      "25/25 [==============================] - 10s 408ms/step - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0256 - val_mean_squared_error: 0.0256\n",
      "Epoch 74/100\n",
      "25/25 [==============================] - 7s 293ms/step - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0257 - val_mean_squared_error: 0.0257\n",
      "Epoch 75/100\n",
      "25/25 [==============================] - 9s 362ms/step - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0253 - val_mean_squared_error: 0.0253\n",
      "Epoch 76/100\n",
      "25/25 [==============================] - 7s 268ms/step - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0256 - val_mean_squared_error: 0.0256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "25/25 [==============================] - 7s 266ms/step - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0252 - val_mean_squared_error: 0.0252\n",
      "Epoch 78/100\n",
      "25/25 [==============================] - 9s 356ms/step - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0257 - val_mean_squared_error: 0.0257\n",
      "Epoch 79/100\n",
      "25/25 [==============================] - 7s 283ms/step - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0258 - val_mean_squared_error: 0.0258\n",
      "Epoch 80/100\n",
      "25/25 [==============================] - 11s 448ms/step - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
      "Epoch 81/100\n",
      "25/25 [==============================] - 6s 259ms/step - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0291 - val_mean_squared_error: 0.0291\n",
      "Epoch 82/100\n",
      "25/25 [==============================] - 9s 385ms/step - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0258 - val_mean_squared_error: 0.0258\n",
      "Epoch 83/100\n",
      "25/25 [==============================] - 6s 257ms/step - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0268 - val_mean_squared_error: 0.0268\n",
      "Epoch 84/100\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0268 - val_mean_squared_error: 0.0268\n",
      "Epoch 85/100\n",
      "25/25 [==============================] - 7s 287ms/step - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0270 - val_mean_squared_error: 0.0270\n",
      "Epoch 86/100\n",
      "25/25 [==============================] - 10s 404ms/step - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0284 - val_mean_squared_error: 0.0284\n",
      "Epoch 87/100\n",
      "25/25 [==============================] - 6s 243ms/step - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
      "Epoch 88/100\n",
      "25/25 [==============================] - 9s 379ms/step - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0275 - val_mean_squared_error: 0.0275\n",
      "Epoch 89/100\n",
      "25/25 [==============================] - 6s 254ms/step - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0257 - val_mean_squared_error: 0.0257\n",
      "Epoch 90/100\n",
      "25/25 [==============================] - 8s 318ms/step - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0278 - val_mean_squared_error: 0.0278\n",
      "Epoch 91/100\n",
      "25/25 [==============================] - 8s 321ms/step - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0273 - val_mean_squared_error: 0.0273\n",
      "Epoch 92/100\n",
      "25/25 [==============================] - 8s 312ms/step - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
      "Epoch 93/100\n",
      "25/25 [==============================] - 8s 330ms/step - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0264 - val_mean_squared_error: 0.0264\n",
      "Epoch 94/100\n",
      "25/25 [==============================] - 11s 459ms/step - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0270 - val_mean_squared_error: 0.0270\n",
      "Epoch 95/100\n",
      "25/25 [==============================] - 8s 335ms/step - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0261 - val_mean_squared_error: 0.0261\n",
      "Epoch 96/100\n",
      "25/25 [==============================] - 9s 346ms/step - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0271 - val_mean_squared_error: 0.0271\n",
      "Epoch 97/100\n",
      "25/25 [==============================] - 11s 430ms/step - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0275 - val_mean_squared_error: 0.0275\n",
      "Epoch 98/100\n",
      "25/25 [==============================] - 8s 328ms/step - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0266 - val_mean_squared_error: 0.0266\n",
      "Epoch 99/100\n",
      "25/25 [==============================] - 10s 414ms/step - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0263 - val_mean_squared_error: 0.0263\n",
      "Epoch 100/100\n",
      "25/25 [==============================] - 9s 349ms/step - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0252 - val_mean_squared_error: 0.0252\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'custom_functions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 96\u001b[0m\n\u001b[0;32m     92\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(x_train, y_train, validation_data\u001b[38;5;241m=\u001b[39m(x_val, y_val), batch_size\u001b[38;5;241m=\u001b[39mbatch_size, epochs\u001b[38;5;241m=\u001b[39mnum_epochs, steps_per_epoch\u001b[38;5;241m=\u001b[39msteps_per_epoch, callbacks\u001b[38;5;241m=\u001b[39m[checkpoint, plot_history, tensorboard_callback])\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Get prediction for the specified training sample\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# plot_prediction_dev = PlotGuess(np.dstack((x_train[0], y_train[0])))\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# custom_functions.plot_prediction(model, sample_iterative[:, :, 0:2], sample_iterative[:, :, 2:4])\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m \u001b[43mcustom_functions\u001b[49m\u001b[38;5;241m.\u001b[39mplot_prediction(model, x_train[\u001b[38;5;241m0\u001b[39m], y_train[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m x_train, y_train, x_val, y_val\n",
      "\u001b[1;31mNameError\u001b[0m: name 'custom_functions' is not defined"
     ]
    }
   ],
   "source": [
    "for folder in selected_folders:\n",
    "    keras.backend.clear_session()\n",
    "    data_folder = directory + folder\n",
    "    print(\"data_folder: \", data_folder)\n",
    "\n",
    "    # X1 = np.load(os.path.join(data_folder, 'X1.npy'))\n",
    "    # X2 = np.load(os.path.join(data_folder, 'X2.npy'))\n",
    "    # E_inc = np.load(os.path.join(data_folder, 'E_inc.npy'))\n",
    "    # ZH_inc = np.load(os.path.join(data_folder, 'ZH_inc.npy'))\n",
    "\n",
    "    if not os.path.exists(data_folder + \"_x_train.npy\"):\n",
    "        print(\"Creating Data Splits\")\n",
    "        files_list = [f for f in os.listdir(data_folder) if f.endswith('o.npy') and \"_info\" not in f and f.startswith(\"instance_\")]\n",
    "        print(\"len(files_list)\", len(files_list))\n",
    "        file_list = files_list\n",
    "        # file_list = random.sample(files_list, sample_count)\n",
    "        print(\"len(file_list)\", len(file_list))\n",
    "        train_val_list, test_list = train_test_split(file_list, test_size=0.2, random_state=42)\n",
    "        train_list, val_list = train_test_split(train_val_list, test_size=0.2, random_state=42)\n",
    "        x_train, y_train = custom_functions.prescient2DL_data(data_folder, train_list, N1, N2)\n",
    "        np.save(data_folder + '_x_train', x_train)\n",
    "        np.save(data_folder + '_y_train', y_train)\n",
    "        print(\"sets created: training\")\n",
    "        x_val, y_val = custom_functions.prescient2DL_data(data_folder, test_list, N1, N2)\n",
    "        np.save(data_folder + '_x_val', x_val)\n",
    "        np.save(data_folder + '_y_val', y_val)\n",
    "        print(\"sets created: validation\")\n",
    "        x_test, y_test = custom_functions.prescient2DL_data(data_folder, val_list, N1, N2)\n",
    "        np.save(data_folder + '_x_test', x_test)\n",
    "        np.save(data_folder + '_y_test', y_test)\n",
    "        print(\"sets created: test\")\n",
    "    else:\n",
    "        print(\"Loading Data Splits\")\n",
    "        x_train = np.load(data_folder + '_x_train.npy')\n",
    "        y_train = np.load(data_folder + '_y_train.npy')\n",
    "        print(\"sets loaded: train\")\n",
    "        x_val = np.load(data_folder + '_x_val.npy')\n",
    "        y_val = np.load(data_folder + '_y_val.npy')\n",
    "        print(\"sets loaded: validation\")\n",
    "        x_test = np.load(data_folder + '_x_test.npy')\n",
    "        y_test = np.load(data_folder + '_y_test.npy')\n",
    "        print(\"sets loaded: test\")\n",
    "\n",
    "    # Determine the total number of samples in the training dataset\n",
    "    total_samples = len(x_train)\n",
    "    print(\"samples training: \", len(x_train))\n",
    "    print(\"samples testing: \", len(x_test))\n",
    "    print(\"samples validation: \", len(x_val))\n",
    "\n",
    "    # Calculate the number of steps per epoch\n",
    "    steps_per_epoch = total_samples // batch_size\n",
    "    print('steps_per_epoch:', steps_per_epoch)\n",
    "    \n",
    "    # Create the U-Net model\n",
    "    model = custom_architectures.get_model(input_shape)\n",
    "    # model = custom_functions.unet_elu(input_shape)\n",
    "    # write to disk\n",
    "    model.summary()\n",
    "    visualkeras.layered_view(model, to_file='.\\\\doc\\\\code_doc\\\\visualkeras.png', legend=True)\n",
    "    plot_model(model, to_file='.\\\\doc\\\\code_doc\\\\model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    # model.compile(optimizer='adam', loss='mean_squared_error', metrics=[MeanSquaredError(), MeanAbsoluteError(), MeanAbsolutePercentageError()])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=MeanSquaredError())\n",
    "    len(model.layers)\n",
    "\n",
    "    # Define the checkpoint callback\n",
    "    checkpoint = ModelCheckpoint('model_checkpoint.h5', monitor='val_loss', save_best_only=True)\n",
    "    plot_history = PlotTrainingHistory()\n",
    "\n",
    "    # Load the saved model\n",
    "    if os.path.exists(os.getcwd() + '\\\\' + 'model_checkpoint.h5'):\n",
    "        print(\"File exists!\")\n",
    "        model = load_model('model_checkpoint.h5')\n",
    "        # Need to recompile the model\n",
    "        # model.compile(optimizer='adam', loss='mean_squared_error', metrics=[MeanSquaredError(), MeanAbsoluteError(), MeanAbsolutePercentageError()])\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error', metrics=MeanSquaredError())\n",
    "\n",
    "    # Load the training history\n",
    "    if os.path.exists(os.getcwd() + '\\\\' + 'training_history.pkl'):\n",
    "        print(\"File exists!\")\n",
    "        with open('training_history.pkl', 'rb') as file:\n",
    "            history = pickle.load(file)\n",
    "        initial_epoch = len(history['loss'])\n",
    "    \n",
    "    # if os.path.exists('model_checkpoint.h5') and os.path.exists('training_history.pkl'):\n",
    "    #     if num_epochs < len(history['loss']):\n",
    "    #         history = model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=batch_size, epochs=num_epochs, steps_per_epoch=steps_per_epoch, initial_epoch=len(history['loss']), callbacks=[checkpoint, plot_history])\n",
    "    #         print(\"Running with history!\")\n",
    "    #     print(\"Already complete epochs!\")\n",
    "    # else:\n",
    "    #     history = model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=batch_size, epochs=num_epochs, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint, plot_history])\n",
    "    history = model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=batch_size, epochs=num_epochs, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint, plot_history, tensorboard_callback])\n",
    "    # Get prediction for the specified training sample\n",
    "    # plot_prediction_dev = PlotGuess(np.dstack((x_train[0], y_train[0])))\n",
    "    # custom_functions.plot_prediction(model, sample_iterative[:, :, 0:2], sample_iterative[:, :, 2:4])\n",
    "    custom_functions.plot_prediction(model, x_train[0], y_train[0])\n",
    "    del x_train, y_train, x_val, y_val\n",
    "    # x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualkeras.layered_view(model, to_file='output.png').show() # write and show\n",
    "# # visualkeras.layered_view(model).show() # display using your system viewer\n",
    "model = custom_architectures.get_model(input_shape)\n",
    "# model = custom_architectures.unet_elu(input_shape)\n",
    "# visualkeras.layered_view(model, to_file='output.png', legend=True) # write to disk\n",
    "# font = ImageFont.truetype(\"arial.ttf\", 32)  # using comic sans is strictly prohibited!\n",
    "# visualkeras.layered_view(model, legend=True, font=font).show()  # font is optional!\n",
    "# visualkeras.layered_view(model, draw_volume=False, legend=True).show()\n",
    "# plot_model(model, to_file='.\\\\doc\\\\code_doc\\\\model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "model = load_model('model_checkpoint.h5')\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error', metrics=[MeanSquaredError(), MeanAbsoluteError(), MeanAbsolutePercentageError()])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=MeanSquaredError())\n",
    "\n",
    "# Evaluate the model using your test dataset\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "# Print the evaluation results\n",
    "print('Test loss:', score[0])\n",
    "print('Test mean absolute error:', score[1])\n",
    "\n",
    "# Select an input from the test set\n",
    "custom_functions.plot_prediction(model, x_test[0], y_test[0])\n",
    "custom_functions.plot_prediction(model, x_test[2], y_test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wqSAZExy6xV"
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "def create_model():\n",
    "  return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28), name='layers_flatten'),\n",
    "    tf.keras.layers.Dense(512, activation='relu', name='layers_dense'),\n",
    "    tf.keras.layers.Dropout(0.2, name='layers_dropout'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax', name='layers_dense_2')\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5pr9vuHVgXY"
   },
   "source": [
    "Using the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset as the example, normalize the data and write a function that creates a simple Keras model for classifying the images into 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKUjdIoV87um"
   },
   "source": [
    "## Using TensorBoard with Keras Model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CL_lxdn8-Sv"
   },
   "source": [
    "When training with Keras's [Model.fit()](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#fit), adding the `tf.keras.callbacks.TensorBoard` callback ensures that logs are created and stored. Additionally, enable histogram computation every epoch with `histogram_freq=1` (this is off by default)\n",
    "\n",
    "Place the logs in a timestamped subdirectory to allow easy selection of different training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAQThq539CEJ"
   },
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(x=x_train, \n",
    "          y=y_train, \n",
    "          epochs=5, \n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asjGpmD09dRl"
   },
   "source": [
    "Start TensorBoard through the command line or within a notebook experience. The two interfaces are generally the same. In notebooks, use the `%tensorboard` line magic. On the command line, run the same command without \"%\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4UKgTLb9fKI"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCsoUNb6YhGc"
   },
   "source": [
    "<!-- <img class=\"tfo-display-only-on-site\" src=\"https://github.com/tensorflow/tensorboard/blob/master/docs/images/quickstart_model_fit.png?raw=1\"/> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gi4PaRm39of2"
   },
   "source": [
    "A brief overview of the visualizations created in this example and the dashboards (tabs in top navigation bar) where they can be found:\n",
    "\n",
    "* **Scalars** show how the loss and metrics change with every epoch. You can use them to also track training speed, learning rate, and other scalar values. Scalars can be found in the **Time Series** or **Scalars** dashboards.\n",
    "* **Graphs** help you visualize your model. In this case, the Keras graph of layers is shown which can help you ensure it is built correctly. Graphs can be found in the **Graphs** dashboard.\n",
    "* **Histograms** and **Distributions** show the distribution of a Tensor over time. This can be useful to visualize weights and biases and verify that they are changing in an expected way. Histograms can be found in the **Time Series** or **Histograms** dashboards. Distributions can be found in the **Distributions** dashboard.\n",
    "\n",
    "Additional TensorBoard dashboards are automatically enabled when you log other types of data. For example, the Keras TensorBoard callback lets you log images and embeddings as well. You can see what other dashboards are available in TensorBoard by clicking on the \"inactive\" dropdown towards the top right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nB718NOH95yG"
   },
   "source": [
    "## Using TensorBoard with other methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKNt0nWs-Ekt"
   },
   "source": [
    "When training with methods such as [`tf.GradientTape()`](https://www.tensorflow.org/api_docs/python/tf/GradientTape), use `tf.summary` to log the required information.\n",
    "\n",
    "Use the same dataset as above, but convert it to `tf.data.Dataset` to take advantage of batching capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnHx4DsMezy1"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(60000).batch(64)\n",
    "test_dataset = test_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzpmTmJafJ10"
   },
   "source": [
    "The training code follows the [advanced quickstart](https://www.tensorflow.org/tutorials/quickstart/advanced) tutorial, but shows how to log metrics to TensorBoard. Choose loss and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2Y5-aPbAANs"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKhIIDj9Hbfy"
   },
   "source": [
    "Create stateful metrics that can be used to accumulate values during training and logged at any point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jD0tEWrgH0TL"
   },
   "outputs": [],
   "source": [
    "# Define our metrics\n",
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
    "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szw_KrgOg-OT"
   },
   "source": [
    "Define the training and test functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTWcJO35IJgK"
   },
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, x_train, y_train):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(x_train, training=True)\n",
    "    loss = loss_object(y_train, predictions)\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(y_train, predictions)\n",
    "\n",
    "def test_step(model, x_test, y_test):\n",
    "  predictions = model(x_test)\n",
    "  loss = loss_object(y_test, predictions)\n",
    "\n",
    "  test_loss(loss)\n",
    "  test_accuracy(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nucPZBKPJR3A"
   },
   "source": [
    "Set up summary writers to write the summaries to disk in a different logs directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Qp-exmbWf4w"
   },
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgUJgDdKWUKF"
   },
   "source": [
    "Start training. Use `tf.summary.scalar()` to log metrics (loss and accuracy) during training/testing within the scope of the summary writers to write the summaries to disk. You have control over which metrics to log and how often to do it. Other `tf.summary` functions enable logging other types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odWvHPpKJvb_"
   },
   "outputs": [],
   "source": [
    "model = create_model() # reset our model\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  for (x_train, y_train) in train_dataset:\n",
    "    train_step(model, optimizer, x_train, y_train)\n",
    "  with train_summary_writer.as_default():\n",
    "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "\n",
    "  for (x_test, y_test) in test_dataset:\n",
    "    test_step(model, x_test, y_test)\n",
    "  with test_summary_writer.as_default():\n",
    "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "  \n",
    "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "  print (template.format(epoch+1,\n",
    "                         train_loss.result(), \n",
    "                         train_accuracy.result()*100,\n",
    "                         test_loss.result(), \n",
    "                         test_accuracy.result()*100))\n",
    "\n",
    "  # Reset metrics every epoch\n",
    "  train_loss.reset_states()\n",
    "  test_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JikosQ84fzcA"
   },
   "source": [
    "Open TensorBoard again, this time pointing it at the new log directory. We could have also started TensorBoard to monitor training while it progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Iue509kgOyE"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/gradient_tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVpnilhEgQXk"
   },
   "source": [
    "<!-- <img class=\"tfo-display-only-on-site\" src=\"https://github.com/tensorflow/tensorboard/blob/master/docs/images/quickstart_gradient_tape.png?raw=1\"/> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozbwXgPIkCKV"
   },
   "source": [
    "That's it! You have now seen how to use TensorBoard both through the Keras callback and through `tf.summary` for more custom scenarios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsowjhkBdkbK"
   },
   "source": [
    "## TensorBoard.dev: Host and share your ML experiment results\n",
    "\n",
    "[TensorBoard.dev](https://tensorboard.dev) is a free public service that enables you to upload your TensorBoard logs and get a permalink that can be shared with everyone in academic papers, blog posts, social media, etc.  This can enable better reproducibility and collaboration.\n",
    "\n",
    "To use TensorBoard.dev, run the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3nupQL24E5E"
   },
   "outputs": [],
   "source": [
    "!tensorboard dev upload \\\n",
    "  --logdir logs/fit \\\n",
    "  --name \"(optional) My latest experiment\" \\\n",
    "  --description \"(optional) Simple comparison of several hyperparameters\" \\\n",
    "  --one_shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAgEh_Ow4EX6"
   },
   "source": [
    "Note that this invocation uses the exclamation prefix (`!`) to invoke the shell\n",
    "rather than the percent prefix (`%`) to invoke the colab magic.  When invoking this command from the command line there is no need for either prefix.\n",
    "\n",
    "View an example [here](https://tensorboard.dev/experiment/EDZb7XgKSBKo6Gznh3i8hg/#scalars).\n",
    "\n",
    "For more details on how to use TensorBoard.dev, see https://tensorboard.dev/#get-started"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "get_started.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
